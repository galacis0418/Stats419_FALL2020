source_url( include.me );
path.to.nascent = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/unit_02_confirmatory_data_analysis/nascent/";
folder.nlp = "nlp/";
path.to.nlp = paste0(path.to.nascent, folder.nlp);
###### UPDATES TO dataframe subset function ######
# inflation adjustments for NA ... and improvements on subsetting
include.me = paste0(path.github, "humanVerseWSU/R/functions-dataframe.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-inflation.R");
source_url( include.me );
# library(devtools);
# install_github("MonteShaffer/imdb/imdb"); # choose #3 to humanVerseWSU
# detach(package:imdb);
library(imdb);
packageVersion("imdb");  # ‘0.1.1’
# ?loadDataIMDB
imdb::loadDataIMDB();
names(imdb.data);
humanVerseWSU::loadInflationData();
library(KernSmooth)
normalDiagnosticPlot = function(x,  normalityTest=TRUE,
showDensity=TRUE,
showNormal=TRUE,
showSDs=FALSE,
showAxis=TRUE
)
{
xx = na.omit(x);
x.stats = doStatsSummary(x);
# x.table = table(x);
# library(KernSmooth); # install.packages("KernSmooth", dependencies=TRUE);
# bin.count = dpih(xx);
# mybreaks = 100 * bin.count;
mxlim = c(x.stats$mean - 3.5 * x.stats$sd ,
x.stats$mean + 3.5 * x.stats$sd );
h = hist(xx, breaks="Sturges", plot=F);
mylim = c(0, max(h$counts));
myMain = paste0(  "Histogram (mean: ",
round(x.stats$mean,digits=3),
", sd: ",
round(x.stats$sd,digits=3),
")"
);
mxlab = "";
if(normalityTest)
{
isNormal = NULL;
if(x.stats$shapiro.is.normal$`0.10`) { isNormal = 0.10; }
if(x.stats$shapiro.is.normal$`0.05`) { isNormal = 0.05; }
if(x.stats$shapiro.is.normal$`0.01`) { isNormal = 0.01; }
isNormalResult = FALSE;
if(!is.null(isNormal)) { isNormalResult = TRUE;}
if(is.null(isNormal)) { isNormal = 0.05;}
mxlab = paste0("Shapiro Normality test at (alpha = ",
isNormal, ") is ... ",isNormalResult);
}
### Histogram
hist(xx, breaks="Sturges",  xlim=mxlim, ylim=mylim,
xlab=mxlab, xaxt='n', main=myMain);
if(showDensity)
{
par(new=T); # overlay
### Density Plot (remember first reading?)
plot( density(xx, kernel="epanechnikov") ,
xlim=mxlim,
main="",
xlab="",
ylab="",
xaxt='n',
yaxt='n'
);
}
if(showNormal)
{
par(new=T); # overlay
### Normal Curve
xt = seq(-3.5,3.5, length=100);
yt = dnorm(xt);
plot( xt, yt,
type="l",
lwd=2,
col = "red",
axes=F,
xlab="",
ylab=""
);
}
if(showSDs)
{
### vertical lines at sd's of data ...
abline(v=x.stats$mean,lwd=4,col="blue");
abline(v=x.stats$mean - 1 * x.stats$sd , col="green",lwd=3);
abline(v=x.stats$mean + 1 * x.stats$sd , col="green",lwd=3);
abline(v=x.stats$mean - 2 * x.stats$sd , col="green",lwd=2);
abline(v=x.stats$mean + 2 * x.stats$sd , col="green",lwd=2);
abline(v=x.stats$mean - 3 * x.stats$sd , col="green",lwd=1);
abline(v=x.stats$mean + 3 * x.stats$sd , col="green",lwd=1);
}
if(showAxis)
{
### axis labels showing the ability to use expression
axis(1, at = -3:3, labels = c( expression("-3"~hat(sigma) ), expression("-2"~sigma ), expression("-1"~hat(s) ), expression(bar(x)), expression("1"~hat(s) ), "2s", c( expression("3"~hat(sigma) ))) );
#axis(1, at = -3:3, labels = c("-3s", "-2s", "-1s", "hat(mu)", "1s", "2s", "3s"))
}
}
will.clean <- removeNAsFromDataFrame(will.movies)
path.project <- "C:/Users/Galac/Desktop/git419/Stats419_FALL2020/Final/"
path.tables = paste0(path.project, "tables/")
createDirRecursive(path.tables)
file.correlation = paste0(path.tables, "Will Smith Correlation.tex")
myData = as.matrix(will.clean[,c("metacritic", "millions", "year")])
buildLatexCorrelationTable(myData,
rotateTable = FALSE,
width.table = .95,
myFile = file.correlation,
myNames = c("metacritic","millions","years"),
myCaption = "Will Smith Correlation Table")
will.clean <- removeNAsFromDataFrame(will.movies)
path.project <- "C:/Users/Galac/Desktop/git419/Stats419_FALL2020/Final/"
path.tables = paste0(path.project, "tables/")
createDirRecursive(path.tables)
file.correlation = paste0(path.tables, "Will-Smith-corr.tex")
myData = as.matrix(will.clean[,c("metacritic", "millions", "year")])
buildLatexCorrelationTable(myData,
rotateTable = FALSE,
width.table = .95,
myFile = file.correlation,
myNames = c("metacritic","millions","years"),
myCaption = "Will Smith Correlation Table")
will.clean <- removeNAsFromDataFrame(will.movies)
path.project <- "C:/Users/Galac/Desktop/git419/Stats419_FALL2020/Final/"
path.tables = paste0(path.project, "tables/")
createDirRecursive(path.tables)
file.correlation = paste0(path.tables, "Will-Smith-corr.tex")
myData = as.matrix(will.clean[,c("metacritic", "millions", "year")])
buildLatexCorrelationTable(myData,
rotateTable = FALSE,
width.table = .95,
myFile = file.correlation,
myNames = c("metacritic","millions","years"),
myCaption = "Will Smith Correlation Table");
library(devtools);
library(humanVerseWSU);
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";
include.me = paste0(path.github, "misc/functions-nlp.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-str.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-stack.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-pos.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-encryption.R");
source_url( include.me );
source_url( paste0(path.humanVerseWSU,"master/misc/functions-project-measure.R") );
path.to.nascent = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/unit_02_confirmatory_data_analysis/nascent/";
folder.nlp = "nlp/";
path.to.nlp = paste0(path.to.nascent, folder.nlp);
###### UPDATES TO dataframe subset function ######
# inflation adjustments for NA ... and improvements on subsetting
include.me = paste0(path.github, "humanVerseWSU/R/functions-dataframe.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-inflation.R");
source_url( include.me );
library(devtools);
library(humanVerseWSU);
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";
include.me = paste0(path.github, "misc/functions-nlp.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-str.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-stack.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-pos.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-encryption.R");
source_url( include.me );
source_url( paste0(path.humanVerseWSU,"master/misc/functions-project-measure.R") );
path.to.nascent = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/unit_02_confirmatory_data_analysis/nascent/";
folder.nlp = "nlp/";
path.to.nlp = paste0(path.to.nascent, folder.nlp);
###### UPDATES TO dataframe subset function ######
# inflation adjustments for NA ... and improvements on subsetting
include.me = paste0(path.github, "humanVerseWSU/R/functions-dataframe.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-inflation.R");
source_url( include.me );
# library(devtools);
# install_github("MonteShaffer/imdb/imdb"); # choose #3 to humanVerseWSU
# detach(package:imdb);
library(imdb);
packageVersion("imdb");  # ‘0.1.1’
# ?loadDataIMDB
imdb::loadDataIMDB();
names(imdb.data);
humanVerseWSU::loadInflationData();
library(KernSmooth)
normalDiagnosticPlot = function(x,  normalityTest=TRUE,
showDensity=TRUE,
showNormal=TRUE,
showSDs=FALSE,
showAxis=TRUE
)
{
xx = na.omit(x);
x.stats = doStatsSummary(x);
# x.table = table(x);
# library(KernSmooth); # install.packages("KernSmooth", dependencies=TRUE);
# bin.count = dpih(xx);
# mybreaks = 100 * bin.count;
mxlim = c(x.stats$mean - 3.5 * x.stats$sd ,
x.stats$mean + 3.5 * x.stats$sd );
h = hist(xx, breaks="Sturges", plot=F);
mylim = c(0, max(h$counts));
myMain = paste0(  "Histogram (mean: ",
round(x.stats$mean,digits=3),
", sd: ",
round(x.stats$sd,digits=3),
")"
);
mxlab = "";
if(normalityTest)
{
isNormal = NULL;
if(x.stats$shapiro.is.normal$`0.10`) { isNormal = 0.10; }
if(x.stats$shapiro.is.normal$`0.05`) { isNormal = 0.05; }
if(x.stats$shapiro.is.normal$`0.01`) { isNormal = 0.01; }
isNormalResult = FALSE;
if(!is.null(isNormal)) { isNormalResult = TRUE;}
if(is.null(isNormal)) { isNormal = 0.05;}
mxlab = paste0("Shapiro Normality test at (alpha = ",
isNormal, ") is ... ",isNormalResult);
}
### Histogram
hist(xx, breaks="Sturges",  xlim=mxlim, ylim=mylim,
xlab=mxlab, xaxt='n', main=myMain);
if(showDensity)
{
par(new=T); # overlay
### Density Plot (remember first reading?)
plot( density(xx, kernel="epanechnikov") ,
xlim=mxlim,
main="",
xlab="",
ylab="",
xaxt='n',
yaxt='n'
);
}
if(showNormal)
{
par(new=T); # overlay
### Normal Curve
xt = seq(-3.5,3.5, length=100);
yt = dnorm(xt);
plot( xt, yt,
type="l",
lwd=2,
col = "red",
axes=F,
xlab="",
ylab=""
);
}
if(showSDs)
{
### vertical lines at sd's of data ...
abline(v=x.stats$mean,lwd=4,col="blue");
abline(v=x.stats$mean - 1 * x.stats$sd , col="green",lwd=3);
abline(v=x.stats$mean + 1 * x.stats$sd , col="green",lwd=3);
abline(v=x.stats$mean - 2 * x.stats$sd , col="green",lwd=2);
abline(v=x.stats$mean + 2 * x.stats$sd , col="green",lwd=2);
abline(v=x.stats$mean - 3 * x.stats$sd , col="green",lwd=1);
abline(v=x.stats$mean + 3 * x.stats$sd , col="green",lwd=1);
}
if(showAxis)
{
### axis labels showing the ability to use expression
axis(1, at = -3:3, labels = c( expression("-3"~hat(sigma) ), expression("-2"~sigma ), expression("-1"~hat(s) ), expression(bar(x)), expression("1"~hat(s) ), "2s", c( expression("3"~hat(sigma) ))) );
#axis(1, at = -3:3, labels = c("-3s", "-2s", "-1s", "hat(mu)", "1s", "2s", "3s"))
}
}
will.clean <- removeNAsFromDataFrame(will.movies)
path.project <- "C:/Users/Galac/Desktop/git419/Stats419_FALL2020/Final/"
path.tables = paste0(path.project, "tables/")
createDirRecursive(path.tables)
file.correlation = paste0(path.tables, "Will-Smith-corr.tex")
myData = as.matrix(will.clean[,c("metacritic", "millions", "year")])
buildLatexCorrelationTable(myData,
rotateTable = FALSE,
width.table = .95,
myFile = file.correlation,
myNames = c("metacritic","millions","years"),
myCaption = "Will Smith Correlation Table");
will.clean <- removeNAsFromDataFrame(will.movies)
path.project <- "C:/Users/Galac/Desktop/git419/Stats419_FALL2020/Final/"
path.tables = paste0(path.project, "tables/")
createDirRecursive(path.tables)
file.correlation = paste0(path.tables, "Will-Smith-corr.tex")
myData = as.matrix(will.clean[,c("metacritic", "millions", "year")])
buildLatexCorrelationTable(myData,
rotateTable = FALSE,
width.table = .95,
myFile = file.correlation,
myNames = c("metacritic","millions","years"),
myCaption = "Will Smith Correlation Table");
setwd("C:/Users/Galac/Desktop/git419/Stats419_FALL2020/Final")
knitr::opts_chunk$set(echo = TRUE)
wd()
knitr::opts_chunk$set(echo = TRUE)
wd()
knitr::opts_chunk$set(echo = TRUE)
library(devtools);
library(humanVerseWSU);
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";
include.me = paste0(path.github, "misc/functions-nlp.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-str.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-stack.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-pos.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-encryption.R");
source_url( include.me );
include.me = paste0(path.github,"misc/functions-project-measure.R");
source_url( include.me);
path.to.nascent = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/unit_02_confirmatory_data_analysis/nascent/";
folder.nlp = "nlp/";
path.to.nlp = paste0(path.to.nascent, folder.nlp);
###### UPDATES TO dataframe subset function ######
# inflation adjustments for NA ... and improvements on subsetting
include.me = paste0(path.github, "humanVerseWSU/R/functions-dataframe.R");
source_url( include.me );
include.me = paste0(path.github, "humanVerseWSU/R/functions-inflation.R");
source_url( include.me );
# library(devtools);
# install_github("MonteShaffer/imdb/imdb"); # choose #3 to humanVerseWSU
# detach(package:imdb);
library(imdb);
packageVersion("imdb");  # ‘0.1.1’
# ?loadDataIMDB
install_github("MonteShaffer/imdb/imdb")
imdb::loadDataIMDB();
names(imdb.data);
humanVerseWSU::loadInflationData();
library(KernSmooth)
normalDiagnosticPlot = function(x,  normalityTest=TRUE,
showDensity=TRUE,
showNormal=TRUE,
showSDs=FALSE,
showAxis=TRUE
)
{
xx = na.omit(x);
x.stats = doStatsSummary(x);
# x.table = table(x);
# library(KernSmooth); # install.packages("KernSmooth", dependencies=TRUE);
# bin.count = dpih(xx);
# mybreaks = 100 * bin.count;
mxlim = c(x.stats$mean - 3.5 * x.stats$sd ,
x.stats$mean + 3.5 * x.stats$sd );
h = hist(xx, breaks="Sturges", plot=F);
mylim = c(0, max(h$counts));
myMain = paste0(  "Histogram (mean: ",
round(x.stats$mean,digits=3),
", sd: ",
round(x.stats$sd,digits=3),
")"
);
mxlab = "";
if(normalityTest)
{
isNormal = NULL;
if(x.stats$shapiro.is.normal$`0.10`) { isNormal = 0.10; }
if(x.stats$shapiro.is.normal$`0.05`) { isNormal = 0.05; }
if(x.stats$shapiro.is.normal$`0.01`) { isNormal = 0.01; }
isNormalResult = FALSE;
if(!is.null(isNormal)) { isNormalResult = TRUE;}
if(is.null(isNormal)) { isNormal = 0.05;}
mxlab = paste0("Shapiro Normality test at (alpha = ",
isNormal, ") is ... ",isNormalResult);
}
### Histogram
hist(xx, breaks="Sturges",  xlim=mxlim, ylim=mylim,
xlab=mxlab, xaxt='n', main=myMain);
if(showDensity)
{
par(new=T); # overlay
### Density Plot (remember first reading?)
plot( density(xx, kernel="epanechnikov") ,
xlim=mxlim,
main="",
xlab="",
ylab="",
xaxt='n',
yaxt='n'
);
}
if(showNormal)
{
par(new=T); # overlay
### Normal Curve
xt = seq(-3.5,3.5, length=100);
yt = dnorm(xt);
plot( xt, yt,
type="l",
lwd=2,
col = "red",
axes=F,
xlab="",
ylab=""
);
}
if(showSDs)
{
### vertical lines at sd's of data ...
abline(v=x.stats$mean,lwd=4,col="blue");
abline(v=x.stats$mean - 1 * x.stats$sd , col="green",lwd=3);
abline(v=x.stats$mean + 1 * x.stats$sd , col="green",lwd=3);
abline(v=x.stats$mean - 2 * x.stats$sd , col="green",lwd=2);
abline(v=x.stats$mean + 2 * x.stats$sd , col="green",lwd=2);
abline(v=x.stats$mean - 3 * x.stats$sd , col="green",lwd=1);
abline(v=x.stats$mean + 3 * x.stats$sd , col="green",lwd=1);
}
if(showAxis)
{
### axis labels showing the ability to use expression
axis(1, at = -3:3, labels = c( expression("-3"~hat(sigma) ), expression("-2"~sigma ), expression("-1"~hat(s) ), expression(bar(x)), expression("1"~hat(s) ), "2s", c( expression("3"~hat(sigma) ))) );
#axis(1, at = -3:3, labels = c("-3s", "-2s", "-1s", "hat(mu)", "1s", "2s", "3s"))
}
}
library(dplyr)
myWill.df = will$movies.50
WillRatings = myWill.df$metacritic
myDenzel.df = denzel$movies.50
denzelTop = myDenzel.df
DMovieRatings = myDenzel.df$metacritic
WGross = myWill.df$millions
DGross = myDenzel.df$millions
normalDiagnosticPlot(WGross)
normalDiagnosticPlot(DGross)
hist(WGross, main = "Histogram of Will Smith movie Grosses")
hist(DGross, main = "Histogram of Denzels movie Grosses")
plot(WGross, DGross)
reg.n = lm(DGross ~ WGross)
abline(reg.n)
abline(reg.n)
t.test(WGross, DGross)
Wyear = myWill.df$year
Dyear = myDenzel.df$year
plot(WillRatings, Wyear)
reg.n = lm(Wyear ~ WillRatings)
abline(reg.n)
abline(reg.n)
plot(Dyear, DMovieRatings)
reg.n = lm(DMovieRatings ~ Dyear)
abline(reg.n)
abline(reg.n)
plot(Wyear, WGross)
reg.n = lm(WGross ~ Wyear, col = "red")
abline(reg.n)
abline(reg.n)
plot(Dyear, DGross)
reg.n = lm(DGross ~ Dyear)
abline(reg.n)
abline(reg.n)
summary(will$movies.50)
summary(denzel$movies.50)
willReviews = will$movies.50
Wreviews = willReviews$ratings
DenzelReviews = denzel$movies.50
DReviews = DenzelReviews$ratings
plot(Wreviews, type = "l", lwd = 2, col = "red")
plot(DReviews, type = "l", lwd = 2, col = "red")
hist(Wreviews)
hist(DReviews)
plot(Wreviews, DReviews)
reg.n = lm(DReviews ~ Wreviews)
abline(reg.n)
abline(reg.n)
normalDiagnosticPlot(willReviews$ratings)
normalDiagnosticPlot(DenzelReviews$ratings)
t.test(Wreviews, DReviews)
library(gtsummary)
library(dplyr)
myWill.df = will$movies.50
WillYears= myWill.df$year
myDenzel.df = denzel$movies.50
denzelyear = myDenzel.df$year
plot(WillYears, denzelyear)
reg.n = lm(denzelyear ~ WillYears)
abline(reg.n)
abline(reg.n)
WGross = myWill.df$millions
DGross = myDenzel.df$millions
plot(WillYears, WGross)
reg.n = lm(WGross ~ WillYears)
abline(reg.n)
abline(reg.n)
plot(denzelyear, DGross)
reg.n = lm(DGross ~ denzelyear)
abline(reg.n)
abline(reg.n)
hist(WillYears)
hist(denzelyear)
total <- merge(myWill.df, myDenzel.df, by="year")
total
t.test(WGross, DGross)
#zero = myWill.df[,c("rating","millions")]
summary(myWill.df)
wd()
